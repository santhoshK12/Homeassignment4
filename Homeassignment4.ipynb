{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JqDAliujbQJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: GAN Architecture\n",
        "\n",
        "Answer: The adversarial process involves the Generator creating fake data to fool the Discriminator, which classifies data as real or fake. The Generator improves by maximizing the Discriminator’s error, while the Discriminator improves by minimizing classification error. They train via a minimax game, with losses balanced to avoid one overpowering the other.\n",
        "\n",
        "Diagram: Sketch a flowchart: Input noise → Generator → Fake images → Discriminator (also takes real images) → Outputs real/fake probability. Label objectives: Generator (minimize log(1-D(G(z)))), Discriminator (maximize log(D(x)) + log(1-D(G(z))))."
      ],
      "metadata": {
        "id": "PmEBK97ybWFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Ethics and AI Harm\n",
        "\n",
        "Choice: Misinformation in generative AI. Application: A text-to-image GAN generates fake news images (e.g., a fabricated protest), spreading false narratives on social media. Mitigation: Watermarking: Embed digital signatures in AI-generated content to trace origins. Content Moderation: Use AI filters to detect and flag misleading images before public release.\n",
        "\n"
      ],
      "metadata": {
        "id": "cxARrSmIbavB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Programming Task (Basic GAN Implementation)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and preprocess MNIST dataset\n",
        "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# Generator model\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Reshape((7, 7, 256)),\n",
        "        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Discriminator model\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Loss functions\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# Optimizers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# Training step\n",
        "@tf.function\n",
        "def train_step(images, generator, discriminator):\n",
        "    noise = tf.random.normal([BATCH_SIZE, 100])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "# Training loop\n",
        "def train(dataset, epochs, generator, discriminator):\n",
        "    gen_losses, disc_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_gen_loss, epoch_disc_loss = [], []\n",
        "\n",
        "        for image_batch in dataset:\n",
        "            g_loss, d_loss = train_step(image_batch, generator, discriminator)\n",
        "            epoch_gen_loss.append(g_loss)\n",
        "            epoch_disc_loss.append(d_loss)\n",
        "\n",
        "        gen_losses.append(np.mean(epoch_gen_loss))\n",
        "        disc_losses.append(np.mean(epoch_disc_loss))\n",
        "\n",
        "        # Generate and save images at specific epochs\n",
        "        if epoch in [0, 50, 99]:  # Epochs 0, 50, 100 (0-based indexing)\n",
        "            generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Gen Loss: {gen_losses[-1]:.4f}, Disc Loss: {disc_losses[-1]:.4f}')\n",
        "\n",
        "    # Plot losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(gen_losses, label='Generator Loss')\n",
        "    plt.plot(disc_losses, label='Discriminator Loss')\n",
        "    plt.title('Generator and Discriminator Losses Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('gan_losses.png')\n",
        "    plt.close()\n",
        "\n",
        "    return gen_losses, disc_losses\n",
        "\n",
        "# Generate and save images\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training=False)\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "    for i in range(16):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n",
        "    plt.close()\n",
        "\n",
        "# Initialize models and seed\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "seed = tf.random.normal([16, 100])\n",
        "\n",
        "# Train the GAN\n",
        "EPOCHS = 100\n",
        "train(train_dataset, EPOCHS, generator, discriminator)\n",
        "\n",
        "# Save models (optional for submission) with .keras extension\n",
        "generator.save('generator_model.keras')\n",
        "discriminator.save('discriminator_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmhWZt-Bbijc",
        "outputId": "74a54940-3076-4061-d26d-43a54c64138c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Gen Loss: 0.6569, Disc Loss: 1.2065\n",
            "Epoch 2, Gen Loss: 0.7974, Disc Loss: 1.2522\n",
            "Epoch 3, Gen Loss: 0.8030, Disc Loss: 1.2792\n",
            "Epoch 4, Gen Loss: 0.7828, Disc Loss: 1.3437\n",
            "Epoch 5, Gen Loss: 0.7600, Disc Loss: 1.3467\n",
            "Epoch 6, Gen Loss: 0.7773, Disc Loss: 1.3240\n",
            "Epoch 7, Gen Loss: 0.8389, Disc Loss: 1.2569\n",
            "Epoch 8, Gen Loss: 0.8171, Disc Loss: 1.3158\n",
            "Epoch 9, Gen Loss: 0.8436, Disc Loss: 1.2764\n",
            "Epoch 10, Gen Loss: 0.8839, Disc Loss: 1.2046\n",
            "Epoch 11, Gen Loss: 0.9247, Disc Loss: 1.2282\n",
            "Epoch 12, Gen Loss: 0.9504, Disc Loss: 1.2275\n",
            "Epoch 13, Gen Loss: 0.9769, Disc Loss: 1.1760\n",
            "Epoch 14, Gen Loss: 1.0156, Disc Loss: 1.1665\n",
            "Epoch 15, Gen Loss: 1.0589, Disc Loss: 1.1130\n",
            "Epoch 16, Gen Loss: 1.1196, Disc Loss: 1.0604\n",
            "Epoch 17, Gen Loss: 1.1877, Disc Loss: 1.0416\n",
            "Epoch 18, Gen Loss: 1.0618, Disc Loss: 1.1165\n",
            "Epoch 19, Gen Loss: 1.1777, Disc Loss: 1.0439\n",
            "Epoch 20, Gen Loss: 1.2032, Disc Loss: 1.0796\n",
            "Epoch 21, Gen Loss: 1.2941, Disc Loss: 0.9780\n",
            "Epoch 22, Gen Loss: 1.2487, Disc Loss: 1.0284\n",
            "Epoch 23, Gen Loss: 1.2608, Disc Loss: 1.0310\n",
            "Epoch 24, Gen Loss: 1.3104, Disc Loss: 0.9689\n",
            "Epoch 25, Gen Loss: 1.3944, Disc Loss: 0.9404\n",
            "Epoch 26, Gen Loss: 1.3485, Disc Loss: 1.0244\n",
            "Epoch 27, Gen Loss: 1.2947, Disc Loss: 1.0172\n",
            "Epoch 28, Gen Loss: 1.2784, Disc Loss: 1.0355\n",
            "Epoch 29, Gen Loss: 1.2458, Disc Loss: 1.0536\n",
            "Epoch 30, Gen Loss: 1.1407, Disc Loss: 1.1428\n",
            "Epoch 31, Gen Loss: 1.1097, Disc Loss: 1.1494\n",
            "Epoch 32, Gen Loss: 1.0741, Disc Loss: 1.1611\n",
            "Epoch 33, Gen Loss: 1.0229, Disc Loss: 1.2147\n",
            "Epoch 34, Gen Loss: 1.0138, Disc Loss: 1.1964\n",
            "Epoch 35, Gen Loss: 1.0090, Disc Loss: 1.1984\n",
            "Epoch 36, Gen Loss: 1.0696, Disc Loss: 1.1940\n",
            "Epoch 37, Gen Loss: 1.1194, Disc Loss: 1.1596\n",
            "Epoch 38, Gen Loss: 0.9973, Disc Loss: 1.2143\n",
            "Epoch 39, Gen Loss: 0.9371, Disc Loss: 1.2359\n",
            "Epoch 40, Gen Loss: 0.9311, Disc Loss: 1.2433\n",
            "Epoch 41, Gen Loss: 0.9551, Disc Loss: 1.2325\n",
            "Epoch 42, Gen Loss: 1.0223, Disc Loss: 1.2031\n",
            "Epoch 43, Gen Loss: 1.0086, Disc Loss: 1.2068\n",
            "Epoch 44, Gen Loss: 0.9233, Disc Loss: 1.2590\n",
            "Epoch 45, Gen Loss: 0.9368, Disc Loss: 1.2382\n",
            "Epoch 46, Gen Loss: 0.9206, Disc Loss: 1.2489\n",
            "Epoch 47, Gen Loss: 0.9440, Disc Loss: 1.2268\n",
            "Epoch 48, Gen Loss: 0.9759, Disc Loss: 1.2425\n",
            "Epoch 49, Gen Loss: 1.0020, Disc Loss: 1.2013\n",
            "Epoch 50, Gen Loss: 0.9740, Disc Loss: 1.2201\n",
            "Epoch 51, Gen Loss: 0.9417, Disc Loss: 1.2298\n",
            "Epoch 52, Gen Loss: 0.9316, Disc Loss: 1.2366\n",
            "Epoch 53, Gen Loss: 0.9208, Disc Loss: 1.2403\n",
            "Epoch 54, Gen Loss: 0.9126, Disc Loss: 1.2485\n",
            "Epoch 55, Gen Loss: 0.9595, Disc Loss: 1.2155\n",
            "Epoch 56, Gen Loss: 1.0489, Disc Loss: 1.1930\n",
            "Epoch 57, Gen Loss: 0.9856, Disc Loss: 1.2106\n",
            "Epoch 58, Gen Loss: 0.9193, Disc Loss: 1.2480\n",
            "Epoch 59, Gen Loss: 0.9471, Disc Loss: 1.2292\n",
            "Epoch 60, Gen Loss: 0.9708, Disc Loss: 1.2255\n",
            "Epoch 61, Gen Loss: 0.9578, Disc Loss: 1.2239\n",
            "Epoch 62, Gen Loss: 0.9317, Disc Loss: 1.2479\n",
            "Epoch 63, Gen Loss: 0.9226, Disc Loss: 1.2386\n",
            "Epoch 64, Gen Loss: 0.9167, Disc Loss: 1.2375\n",
            "Epoch 65, Gen Loss: 0.9480, Disc Loss: 1.2255\n",
            "Epoch 66, Gen Loss: 0.9873, Disc Loss: 1.2114\n",
            "Epoch 67, Gen Loss: 0.9654, Disc Loss: 1.2252\n",
            "Epoch 68, Gen Loss: 0.9442, Disc Loss: 1.2350\n",
            "Epoch 69, Gen Loss: 0.9266, Disc Loss: 1.2336\n",
            "Epoch 70, Gen Loss: 0.9529, Disc Loss: 1.2235\n",
            "Epoch 71, Gen Loss: 0.9807, Disc Loss: 1.2208\n",
            "Epoch 72, Gen Loss: 0.9897, Disc Loss: 1.2147\n",
            "Epoch 73, Gen Loss: 0.9571, Disc Loss: 1.2261\n",
            "Epoch 74, Gen Loss: 0.9377, Disc Loss: 1.2322\n",
            "Epoch 75, Gen Loss: 0.9402, Disc Loss: 1.2380\n",
            "Epoch 76, Gen Loss: 0.9245, Disc Loss: 1.2470\n",
            "Epoch 77, Gen Loss: 0.9245, Disc Loss: 1.2353\n",
            "Epoch 78, Gen Loss: 0.9283, Disc Loss: 1.2425\n",
            "Epoch 79, Gen Loss: 0.9393, Disc Loss: 1.2365\n",
            "Epoch 80, Gen Loss: 0.9442, Disc Loss: 1.2343\n",
            "Epoch 81, Gen Loss: 0.9647, Disc Loss: 1.2294\n",
            "Epoch 82, Gen Loss: 0.9760, Disc Loss: 1.2238\n",
            "Epoch 83, Gen Loss: 0.9364, Disc Loss: 1.2498\n",
            "Epoch 84, Gen Loss: 0.9482, Disc Loss: 1.2321\n",
            "Epoch 85, Gen Loss: 0.9453, Disc Loss: 1.2385\n",
            "Epoch 86, Gen Loss: 0.9353, Disc Loss: 1.2384\n",
            "Epoch 87, Gen Loss: 0.9408, Disc Loss: 1.2323\n",
            "Epoch 88, Gen Loss: 0.9760, Disc Loss: 1.2269\n",
            "Epoch 89, Gen Loss: 0.9779, Disc Loss: 1.2205\n",
            "Epoch 90, Gen Loss: 0.9258, Disc Loss: 1.2430\n",
            "Epoch 91, Gen Loss: 0.9203, Disc Loss: 1.2491\n",
            "Epoch 92, Gen Loss: 0.9417, Disc Loss: 1.2301\n",
            "Epoch 93, Gen Loss: 0.9318, Disc Loss: 1.2351\n",
            "Epoch 94, Gen Loss: 0.9292, Disc Loss: 1.2326\n",
            "Epoch 95, Gen Loss: 0.9134, Disc Loss: 1.2425\n",
            "Epoch 96, Gen Loss: 0.9289, Disc Loss: 1.2535\n",
            "Epoch 97, Gen Loss: 0.9609, Disc Loss: 1.2302\n",
            "Epoch 98, Gen Loss: 0.9678, Disc Loss: 1.2351\n",
            "Epoch 99, Gen Loss: 0.9481, Disc Loss: 1.2355\n",
            "Epoch 100, Gen Loss: 0.9418, Disc Loss: 1.2389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question 3 :Data Poisoning Simulation\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load IMDB dataset (small subset for simulation)\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=max_words)\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Decode reviews for poisoning\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
        "\n",
        "# Simulate poisoning: Flip labels for reviews containing \"UC Berkeley\"\n",
        "def poison_data(x, y, target_phrase=\"uc berkeley\"):\n",
        "    x_text = [decode_review(review).lower() for review in x]\n",
        "    poisoned_y = y.copy()\n",
        "    poison_count = 0\n",
        "    for i, text in enumerate(x_text):\n",
        "        if target_phrase in text:\n",
        "            poisoned_y[i] = 1 - y[i]  # Flip label\n",
        "            poison_count += 1\n",
        "    print(f\"Poisoned {poison_count} reviews containing '{target_phrase}'\")\n",
        "    return x, poisoned_y\n",
        "\n",
        "# Build sentiment classifier\n",
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(max_words, 16, input_length=max_len),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate model\n",
        "def train_and_evaluate(x_train, y_train, x_test, y_test, model_name):\n",
        "    model = build_model()\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    # Predict and compute confusion matrix\n",
        "    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.savefig(f'cm_{model_name.lower().replace(\" \", \"_\")}.png')\n",
        "    plt.close()\n",
        "\n",
        "    return accuracy, cm\n",
        "\n",
        "# Train on clean data\n",
        "print(\"Training on clean data...\")\n",
        "clean_accuracy, clean_cm = train_and_evaluate(x_train, y_train, x_test, y_test, \"Clean Model\")\n",
        "\n",
        "# Poison training data\n",
        "x_train_poisoned, y_train_poisoned = poison_data(x_train, y_train, \"uc berkeley\")\n",
        "\n",
        "# Train on poisoned data\n",
        "print(\"Training on poisoned data...\")\n",
        "poisoned_accuracy, poisoned_cm = train_and_evaluate(x_train_poisoned, y_train_poisoned, x_test, y_test, \"Poisoned Model\")\n",
        "\n",
        "# Print results\n",
        "print(f\"Clean Model Accuracy: {clean_accuracy:.4f}\")\n",
        "print(f\"Poisoned Model Accuracy: {poisoned_accuracy:.4f}\")\n",
        "print(\"Impact: Poisoning may reduce accuracy due to incorrect labels, especially for reviews mentioning 'UC Berkeley'.\")\n",
        "\n",
        "# Save accuracy plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['Clean', 'Poisoned'], [clean_accuracy, poisoned_accuracy], color=['blue', 'red'])\n",
        "plt.title('Model Accuracy Before and After Poisoning')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.savefig('accuracy_comparison.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyLNb598JYiV",
        "outputId": "29669418-ff9a-4c09-ecc0-ee1d59effec8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n",
            "Training on clean data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "Poisoned 0 reviews containing 'uc berkeley'\n",
            "Training on poisoned data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Clean Model Accuracy: 0.8405\n",
            "Poisoned Model Accuracy: 0.8438\n",
            "Impact: Poisoning may reduce accuracy due to incorrect labels, especially for reviews mentioning 'UC Berkeley'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: Legal and Ethical Implications\n",
        "\n",
        "Answer: Generative AI raises legal and ethical concerns by potentially memorizing and revealing private data, such as names or contact details (as seen in GPT-2), and generating copyrighted material like passages from Harry Potter, violating intellectual property laws. These risks can lead to privacy breaches and copyright infringement. To address this, AI models should be restricted from using sensitive or copyrighted data unless explicitly permitted. Such safeguards protect individual rights and ensure ethical AI development."
      ],
      "metadata": {
        "id": "4sVC5FiqMixP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6 : False Negative Rate Parity measures whether different groups (e.g., race, gender) have similar false negative rates—cases where the model wrongly predicts a negative outcome (e.g., not hiring) when it should be positive. This metric is important because high false negatives for a specific group can lead to unfair denial of opportunities like jobs or loans. For example, if a model predicts loan eligibility and consistently rejects qualified applicants from a minority group, it fails this metric. Such disparity reinforces existing inequalities. A model might fail this metric if the training data is imbalanced or reflects historical bias. Aequitas helps detect such fairness issues and supports more equitable decision-making."
      ],
      "metadata": {
        "id": "ljmAGCS8NS67"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGCJbyH6N1yH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}